2021-08-30 15:30:59  [ Executor task launch worker for task 1:0 ] - [ ERROR ]  Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.IllegalArgumentException: fk_dt does not exist. Available: order_no, unix_timestamp(loan_success_time, yyyy-MM-dd HH:mm:ss)
	at org.apache.spark.sql.types.StructType.$anonfun$fieldIndex$1(StructType.scala:306)
	at scala.collection.immutable.Map$Map2.getOrElse(Map.scala:193)
	at org.apache.spark.sql.types.StructType.fieldIndex(StructType.scala:305)
	at org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema.fieldIndex(rows.scala:187)
	at org.apache.spark.sql.Row.getAs(Row.scala:367)
	at org.apache.spark.sql.Row.getAs$(Row.scala:367)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getAs(rows.scala:166)
	at org.apache.spark.sql.Row.$anonfun$getValuesMap$1(Row.scala:390)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:273)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:273)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:266)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.Row.getValuesMap(Row.scala:389)
	at org.apache.spark.sql.Row.getValuesMap$(Row.scala:388)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getValuesMap(rows.scala:166)
	at controller.collect.ExportData$.$anonfun$main$1(ExportData.scala:63)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.mapelements_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2021-08-30 15:30:59  [ task-result-getter-1:260 ] - [ ERROR ]  Task 0 in stage 1.0 failed 1 times; aborting job
